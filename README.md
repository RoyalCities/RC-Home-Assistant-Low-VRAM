# ğŸ§  Local AI Voice Assistant Stack (GPU-Accelerated)

> âš ï¸ **Note:** This stack is intended to be run in a **containerized Docker environment**.  
> If you are installing Home Assistant or its components outside Docker (e.g., via Home Assistant OS, Supervised, or Python virtualenv),  
> please refer to the official documentation instead: [Getting Started with Home Assistant](https://www.home-assistant.io/getting-started/)

This repo contains a self-hosted voice assistant stack that runs entirely on your machine â€” no cloud required, no subscriptions, and no surveillance.

It uses Docker Compose to orchestrate services like Home Assistant, Whisper for speech recognition, Piper for text-to-speech, and OpenWakeWord for wake word detection. Each service is GPU-accelerated for fast, low-latency voice interaction.

---

## âš™ï¸ What's Inside

| Service         | Purpose                             | GPU Support | Notes |
|-----------------|-------------------------------------|-------------|-------|
| `homeassistant` | Smart home brain                    | âŒ          | Controls devices and automations |
| `whisper`       | Speech-to-text                      | âœ…          | Whisper model w/ CUDA support |
| `piper`         | Text-to-speech                      | âœ…          | CUDA-accelerated voice synthesis |
| `openwakeword`  | Wake word detection                 | âœ…          | Useful for ESP32/Atom devices |

---


## ğŸ§° Folder Structure

```text
.
â”œâ”€â”€ docker-compose.yml         # Core stack config
â”œâ”€â”€ homeassistant_config/      # HA configuration volume
â”œâ”€â”€ whisper-data/              # Whisper model data
â”œâ”€â”€ piper-data/                # Piper voice data
â””â”€â”€ wakeword-data/             # Custom wake word files
```

Note: 

> â„¹ï¸ Docker will auto-create the volume folders (like `./whisper-data`, `./piper-data`, etc.) if they donâ€™t exist.  
> You donâ€™t need to manually create them â€” theyâ€™ll show up after your first `docker compose up`.

---

## ğŸš€ Quick Start

> Make sure you have Docker and NVIDIA GPU support set up (via `nvidia-container-toolkit`).

1. Clone this repo:
   ```bash
   git clone https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM.git
   cd RC-Home-Assistant-Low-VRAM
   ```

2. Start everything:
   ```bash
   docker compose up -d
   ```

3. Access Home Assistant:
   - Navigate to [http://localhost:8123](http://localhost:8123) in your browser.

---


## ğŸ–¥ï¸ GPU Acceleration

Each AI service uses NVIDIA GPU acceleration via the following Compose directive:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: ["gpu", "utility", "compute"]
```

This speeds up Whisper, Piper, and OpenWakeWord drastically compared to CPU-based inference. This ensures the entire stack hits your GPU.

> ğŸ’¡ Make sure your NVIDIA drivers and `nvidia-docker` runtime are properly installed.

---

## ğŸ”µ Bluetooth Support (Linux Only)

```yaml
devices:
  - "/dev/hci0"
```

This line in the `homeassistant` container enables Bluetooth support â€” useful for presence detection, BLE sensors, or Matter-over-Bluetooth. It only works on Linux hosts.

> ğŸªŸ If you're on **Windows** or **macOS**, you can safely remove this line.

---

## ğŸŒ Network Mode: Host

```yaml
network_mode: host
```

Home Assistant uses host networking to enable local discovery (e.g., mDNS, SSDP, uPnP) for devices like Google Home, Chromecast, and other smart devices.

> âš ï¸ **This setting is only relevant if you're running Home Assistant inside Docker.**  
> If you're running Home Assistant natively (e.g., via Home Assistant OS, Supervised, or in a VM), this is handled automatically â€” you donâ€™t need to configure network mode manually.

> âš ï¸ **Docker limitation on Windows/macOS:**  
> `network_mode: host` only works properly on **Linux**.  
> On other platforms, local device discovery may not work unless you use bridged networking, WSL2, or run HA in a Linux-based VM.

---

## ğŸ”” Wake Word Support (OpenWakeWord vs Voice PE)

This stack includes [`openwakeword`](https://github.com/davidchatting/openWakeWord) to support **custom wake words**, using `.tflite` models trained or downloaded locally.

### ğŸ§  When to Use It

- âœ… You're using **ESP32â€‘S3 devices** like:
  - M5Stack ATOM Echo
  - ESP32â€‘S3â€‘BOXâ€‘3
- âœ… You want to stream audio to HA and use **custom wake words**

### ğŸš« When NOT to Use It

- âŒ You're using a **Home Assistant Voice Preview Edition (Voice PE)** unit  
  > Voice PE uses the **`microWakeWord` engine**, which runs entirely on-device and **only supports built-in wake words** like:
  > - "Okay Nabu"
  > - "Hey Jarvis"
  > - "Hey Mycroft"

Including `openwakeword` future-proofs your setup for advanced use cases, but it's not currently used by Voice PE. Hopefully official support for openwakeword comes at some point in the future to HA Voice Preview.

---

## ğŸ§¼ Customization & Notes

- You can change the Whisper model size (`tiny`, `small`, `medium`) via the `MODEL=` environment variable. The default model I have set up is perfectly capable and ensures minimal VRAM hit but feel free to try different ones.
- Piper supports multiple voice presets â€” I set a default one for the `--voice` argument you change this within the HA GUI itself.
- Feel free to remove services you donâ€™t need (e.g., `openwakeword`) by commenting them out in `docker-compose.yml`.

---

## âš ï¸ Ollama Required (External)

This stack assumes **Ollama is installed and running locally on your host machine**, **outside of Docker**.

> Ollama is not included in this Compose stack.

Make sure Ollama is running and listening on `http://localhost:11434` before starting Home Assistant.  
You will also need to configure your preferred AI model manually within Home Assistant under **Settings â†’ Voice Assistants â†’ LLM Settings**.

- [ğŸ”— Get Ollama](https://ollama.com)
- [ğŸ“– Ollama Docs](https://ollama.com/library)

---

## ğŸ“š Advanced Features & Guides

Once you're up and running, you can add advanced voice features:

### ğŸ§  Persistent Memory (Shortâ€‘ & Longâ€‘Term)

Add memory to your assistant â€” it can remember facts you tell it (*â€œremember that my birthday is January 1stâ€*) and log recent actions (*â€œI launched YouTube on the TVâ€*) for smarter context.

- [ğŸ§  View the Persistent Memory Guide â€º](https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/blob/main/AI-Persistent-Memory.md)

### ğŸ” Multi-Turn Voice: Vocal Daisy Chaining

Add natural follow-up support. For example:

> â€œTurn on the TV.â€  
> â€œThe TV is now on. Would you like to open an app?â€

- [ğŸ” View the Daisy Chaining Guide â€º](https://github.com/RoyalCities/RC-Home-Assistant-Low-VRAM/blob/main/Vocal-Command-Daisy-Chaining.md)

> ğŸ’¡ These modules work independently â€” but together, they create a true multi-turn, memory-capable voice assistant.

---

## ğŸ“„ License

MIT License for code. Any media/design content is CC-BY 4.0 unless noted otherwise.

---

## ğŸ—£ï¸ Credits / Acknowledgements

- [Home Assistant](https://www.home-assistant.io/)
- Special thanks to the Home Assistant Voice team for making local voice a reality.
